from typing import Dict, Any
import asyncio
import concurrent.futures
from src.core.gemini_client import GeminiClient
from src.core.database import DatabaseManager
import json
from ..utils.markdown_utils import (
    format_code_blocks,
    format_summary_to_markdown,
    format_answer_text,
    detect_and_fence_indented_code,
)
from ..flows.mindmap_flow import MindmapFlow

class ContentFlow:
    """å…§å®¹è™•ç†æµç¨‹ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰å…§å®¹åˆ†æã€å•é¡Œç”Ÿæˆå’ŒçŸ¥è­˜é»é—œè¯"""
    
    def __init__(self, gemini_client: GeminiClient, db_manager: DatabaseManager):
        self.gemini = gemini_client
        self.db = db_manager
        from ..utils.file_processor import FileProcessor
        self.file_processor = FileProcessor()
        self.mindmap_flow = MindmapFlow(gemini_client, db_manager)

    @staticmethod
    def _sanitize_question_text(text: str) -> str:
        """ç§»é™¤å¯èƒ½åŒ…å«è§£ç­”æˆ–èªªæ˜æ¨™é¡Œçš„è¡Œï¼Œä½†ä¿ç•™ç¨‹å¼ç¢¼å€å¡Šå…§çš„å…§å®¹"""
        if not text:
            return text
        import re
        pattern = re.compile(r"^\s*(ç­”æ¡ˆ|è§£ç­”|åƒè€ƒç­”æ¡ˆ|å»ºè­°|èªªæ˜|è§£æ)[\s:ï¼š]", re.I)
        lines = text.splitlines()
        sanitized_lines = []
        in_code_block = False

        for line in lines:
            if line.strip().startswith("```"):
                in_code_block = not in_code_block
                sanitized_lines.append(line)
            elif in_code_block:
                sanitized_lines.append(line)
            elif not pattern.match(line):
                sanitized_lines.append(line)

        cleaned = "\n".join(sanitized_lines).strip()
        return detect_and_fence_indented_code(cleaned)

    
    
    def process_file(self, file_path: str, filename: str, suggested_subject: str = None) -> Dict[str, Any]:
        """è™•ç†æª”æ¡ˆçš„çµ±ä¸€å…¥å£é»"""
        try:
            # content is the extracted text from the file
            content, _ = self.file_processor.process_input(file_path)

            # ======================================================================
            # â–¼â–¼â–¼ DEBUG CHECKPOINT 1: æª¢æŸ¥ FileProcessor çš„è¼¸å‡º â–¼â–¼â–¼
            print("\n" + "="*20 + " DEBUG CHECKPOINT 1: AFTER FileProcessor " + "="*20)
            print("--- Raw content extracted from file ---")
            print(content)
            print("="*67 + "\n")
            # â–²â–²â–² DEBUG CHECKPOINT 1 â–²â–²â–²
            # ======================================================================

            # Pass the file_path along with the extracted content
            return self.complete_ai_processing(content, filename, suggested_subject, file_path=file_path)
        except Exception as e:
            print(f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {'success': False, 'error': str(e), 'message': f'æª”æ¡ˆè™•ç†å¤±æ•—: {str(e)}'}
    
    def complete_ai_processing(self, content: str, filename: str, suggested_subject: str = None, source_url: str = None, file_path: str = None) -> Dict[str, Any]:
        """å®Œæ•´ AI è™•ç†æµç¨‹"""
        try:
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, self._run_async_processing(content, filename, suggested_subject, source_url, file_path))
                return future.result()
        except Exception as e:
            print(f"å®Œæ•´ AI è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {'success': False, 'error': str(e), 'message': 'è™•ç†å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦'}
    
    def _extract_answer_string(self, answer_data: Any) -> str:
        """Recursively extracts the answer string from potentially nested answer_data."""
        if isinstance(answer_data, dict):
            if 'answer' in answer_data:
                extracted_answer = self._extract_answer_string(answer_data['answer'])
                if not extracted_answer or "not included in the prompt" in extracted_answer.lower() or "answer is not provided" in extracted_answer.lower():
                    return "ï¼ˆåƒè€ƒç­”æ¡ˆç”Ÿæˆå¤±æ•—æˆ–æœªæä¾›ï¼Œè«‹æª¢æŸ¥åŸå§‹è³‡æ–™æˆ–ç¨å¾Œé‡è©¦ã€‚ï¼‰"
                return extracted_answer
            else:
                return json.dumps(answer_data, ensure_ascii=False)
        elif isinstance(answer_data, list):
            return json.dumps(answer_data, ensure_ascii=False)
        else:
            extracted_answer = str(answer_data)
            if not extracted_answer or "not included in the prompt" in extracted_answer.lower() or "answer is not provided" in extracted_answer.lower():
                return "ï¼ˆåƒè€ƒç­”æ¡ˆç”Ÿæˆå¤±æ•—æˆ–æœªæä¾›ï¼Œè«‹æª¢æŸ¥åŸå§‹è³‡æ–™æˆ–ç¨å¾Œé‡è©¦ã€‚ï¼‰"
            return extracted_answer

    async def _run_async_processing(self, content: str, filename: str, suggested_subject: str = None, source_url: str = None, file_path: str = None) -> Dict[str, Any]:
        """åŸ·è¡Œç•°æ­¥è™•ç†æµç¨‹"""
        try:
            print("ğŸ¤– AI æ­£åœ¨åˆ†æå…§å®¹é¡å‹...")
            parsed_data = await self.gemini.parse_exam_paper(content)

            # ======================================================================
            # â–¼â–¼â–¼ DEBUG CHECKPOINT 2 (å·²ä¿®æ­£) â–¼â–¼â–¼
            print("\n" + "="*20 + " DEBUG CHECKPOINT 2: AFTER parse_exam_paper " + "="*20)
            print("--- Full parsed_data from AI ---")
            print(json.dumps(parsed_data, indent=2, ensure_ascii=False))
            # ä¿®æ­£ï¼šè¿­ä»£ questions åˆ—è¡¨ä¾†å°å‡ºæ¯å€‹ stem
            if parsed_data.get('questions'):
                for i, q_data in enumerate(parsed_data['questions']):
                    stem_text = q_data.get('stem', 'STEM NOT FOUND')
                    print(f"\n--- Extracted 'stem' from Question {i+1} ---")
                    print(stem_text)
            print("="*70 + "\n")
            # â–²â–²â–² DEBUG CHECKPOINT 2 (å·²ä¿®æ­£) â–²â–²â–²
            # ======================================================================
            
            content_type = parsed_data.get('content_type', 'study_material')
            detected_subject = parsed_data.get('subject', suggested_subject or 'å…¶ä»–')
            
            print(f"ğŸ“‹ å…§å®¹åˆ†é¡çµæœï¼š{content_type} ({detected_subject})")
            
            doc_id = self.db.add_document(
                title=filename, 
                content=content, # Extracted text
                subject=detected_subject, 
                source=source_url,
                file_path=file_path # The actual file path
            )
            
            if content_type == 'exam_paper':
                print("ğŸ“ æª¢æ¸¬åˆ°è€ƒé¡Œå…§å®¹ï¼ŒåŸ·è¡Œè€ƒé¡Œè™•ç†æµç¨‹...")
                result = await self._process_exam_content(content, detected_subject, doc_id, parsed_data)
            else:
                print("ğŸ“š æª¢æ¸¬åˆ°å­¸ç¿’è³‡æ–™ï¼ŒåŸ·è¡Œå­¸ç¿’è³‡æ–™è™•ç†æµç¨‹...")
                result = await self._process_study_material(content, detected_subject, doc_id, parsed_data)
            
            if result.get('success'):
                return result
            else:
                return result
                
        except Exception as e:
            print(f"ç•°æ­¥è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise e
    
    async def _process_exam_content(self, content: str, subject: str, doc_id: int, parsed_data: Dict) -> Dict[str, Any]:
        """è€ƒé¡Œè™•ç†æµç¨‹"""
        questions = parsed_data.get('questions', [])
        saved_questions = []
        all_knowledge_points = set()
        
        print(f"ğŸ“ é–‹å§‹è™•ç† {len(questions)} é“è€ƒé¡Œ...")
        
        for i, question_data in enumerate(questions, 1):
            try:
                # ======================================================================
                # â–¼â–¼â–¼ é€™æ˜¯è§£æ±ºæ’ç‰ˆå•é¡Œçš„æœ€çµ‚ä¿®æ­£ï¼ â–¼â–¼â–¼
                # æˆ‘å€‘ä¸å†å‘¼å« _sanitize_question_textï¼Œå› ç‚º stem çš„æ ¼å¼å·²ç¶“æ˜¯å®Œç¾çš„äº†ã€‚
                question_text = question_data.get('stem', '')
                # â–²â–²â–² é€™æ˜¯è§£æ±ºæ’ç‰ˆå•é¡Œçš„æœ€çµ‚ä¿®æ­£ï¼ â–²â–²â–²
                # ======================================================================
                
                if not question_text:
                    continue

                # ======================================================================
                # â–¼â–¼â–¼ DEBUG CHECKPOINT 3: æª¢æŸ¥é€²å…¥ç¬¬äºŒæ¬¡ AI å‘¼å«å‰çš„æœ€çµ‚è³‡æ–™ â–¼â–¼â–¼
                print("\n" + "="*20 + " DEBUG CHECKPOINT 3: BEFORE generate_answer " + "="*20)
                print("--- Final question_text passed to generate the answer ---")
                print(question_text)
                print("="*73 + "\n")
                # â–²â–²â–² DEBUG CHECKPOINT 3 â–²â–²â–²
                # ======================================================================

                # ç›´æ¥ä½¿ç”¨ç´”æ·¨çš„é¡Œå¹¹ç”Ÿæˆç­”æ¡ˆ
                answer_data = await self.gemini.generate_answer(question_text)
                print(f"DEBUG: answer_data type: {type(answer_data)}, value: {answer_data}")
                answer_text = format_answer_text(self._extract_answer_string(answer_data))
                print(f"DEBUG: answer_text type: {type(answer_text)}, value: {answer_text}")
                sources_json = json.dumps(answer_data.get('sources', []), ensure_ascii=False)
                
                question_id = self.db.insert_question(
                    document_id=doc_id,
                    title=question_data.get('title', f'é¡Œç›® {i}'),
                    question_text=format_code_blocks(question_text),
                    answer_text=format_code_blocks(answer_text),
                    answer_sources=sources_json,
                    subject=subject,
                    difficulty=question_data.get('difficulty'),
                    guidance_level=question_data.get('guidance_level')
                )
                print(f"DEBUG: Difficulty: {question_data.get('difficulty')}, Guidance Level: {question_data.get('guidance_level')}")
                
                await self.mindmap_flow.generate_and_save_mindmap(question_id)
                
                knowledge_points = question_data.get('knowledge_points', [])
                for kp_name in knowledge_points:
                    kp_id = self.db.add_or_get_knowledge_point(kp_name.strip(), subject)
                    self.db.link_question_to_knowledge_point(question_id, kp_id)
                    all_knowledge_points.add(kp_name.strip())
                
                saved_questions.append({
                    'id': question_id,
                    'stem': question_text,
                    'answer': answer_text,
                    'sources': answer_data.get('sources', []),
                    'knowledge_points': knowledge_points
                })
            except Exception as e:
                print(f"    è™•ç†ç¬¬ {i} é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        return {
            'success': True,
            'content_type': 'exam_paper',
            'subject': subject,
            'document_id': doc_id,
            'questions': saved_questions,
            'knowledge_points': list(all_knowledge_points),
            'message': f'æˆåŠŸè™•ç†è€ƒé¡Œï¼Œè§£æäº† {len(saved_questions)} é“é¡Œç›®ã€‚'
        }

    async def _process_study_material(self, content: str, subject: str, doc_id: int, parsed_data: Dict) -> Dict[str, Any]:
        """å­¸ç¿’è³‡æ–™è™•ç†æµç¨‹"""
        print("ğŸ“š åŸ·è¡Œå­¸ç¿’è³‡æ–™è™•ç†æµç¨‹...")
        
        # ç”Ÿæˆæ¨¡æ“¬é¡Œ
        generated_questions = await self.gemini.generate_questions_from_text(content, subject)
        saved_questions = []
        all_knowledge_points = set()

        for q_data in generated_questions:
            q_text = self._sanitize_question_text(q_data.get('question', ''))
            answer_data = await self.gemini.generate_answer(q_text)
            answer_text = format_code_blocks(
                format_answer_text(self._extract_answer_string(answer_data))
            )
            sources_json = json.dumps(answer_data.get('sources', []), ensure_ascii=False)

            question_id = self.db.insert_question(
                document_id=doc_id,
                title=q_data.get('title', 'æ¨¡æ“¬é¡Œ'),
                question_text=format_code_blocks(q_text),
                answer_text=answer_text,
                answer_sources=sources_json,
                subject=subject,
                difficulty=q_data.get('difficulty'),
            )
            # ç”Ÿæˆå¿ƒæ™ºåœ–
            mindmap_code = await self.mindmap_flow.generate_and_save_mindmap(question_id)
            
            # å°‡å¿ƒæ™ºåœ–ç¨‹å¼ç¢¼å„²å­˜åˆ° mindmap_code æ¬„ä½
            if mindmap_code:
                self.db.update_question_mindmap(question_id, mindmap_code)

            q_data['answer'] = answer_text
            q_data['sources'] = answer_data.get('sources', [])
            q_data['question'] = q_text
            saved_questions.append({'id': question_id, **q_data})
            
            

            

            for kp_name in q_data.get('knowledge_points', []):
                kp_id = self.db.add_or_get_knowledge_point(kp_name.strip(), subject)
                self.db.link_question_to_knowledge_point(question_id, kp_id)
                all_knowledge_points.add(kp_name.strip())

        # ç”Ÿæˆæ‘˜è¦å’Œæ¸¬é©—
        summary_raw_data = await self.gemini.generate_summary(content)
        # ç¢ºä¿ summary_data æ˜¯å­—å…¸ï¼Œå¦‚æœä¸æ˜¯å‰‡å˜—è©¦è§£æ
        if isinstance(summary_raw_data, str):
            try:
                summary_data = json.loads(summary_raw_data)
            except json.JSONDecodeError:
                summary_data = {} # è§£æå¤±æ•—å‰‡è¨­ç‚ºç©ºå­—å…¸
        else:
            summary_data = summary_raw_data

        quiz_data = await self.gemini.generate_quick_quiz(content, subject)

        # å„²å­˜æ‘˜è¦å’Œæ¸¬é©—
        summary_text = format_summary_to_markdown(summary_data) if summary_data else None
        quiz_text = json.dumps(quiz_data, ensure_ascii=False) if quiz_data else None
        self.db.update_document_summary_and_quiz(doc_id, summary_text, quiz_text)

        return {
            'success': True,
            'content_type': 'study_material',
            'subject': subject,
            'document_id': doc_id,
            'questions': saved_questions,
            'knowledge_points': list(all_knowledge_points),
            'summary': summary_data,
            'quiz': quiz_data,
            'message': f'å­¸ç¿’è³‡æ–™è™•ç†å®Œæˆï¼ç”Ÿæˆäº† {len(saved_questions)} é“æ¨¡æ“¬é¡Œã€‚'
        }