import asyncio
import hashlib
from datetime import datetime
from typing import Dict, Any, List
import os

from ..core.gemini_client import GeminiClient
from ..core.database import DatabaseManager
from ..utils.file_processor import FileProcessor, ContentValidator
from .content_flow import ContentFlow

class InfoFlow:
    """å­¸ç¿’è³‡æ–™è™•ç†æµç¨‹"""
    
    def __init__(self, gemini_client: GeminiClient, db_manager: DatabaseManager, content_processor: ContentFlow):
        self.gemini = gemini_client
        self.db = db_manager
        self.data_dir = "./data"
        os.makedirs(self.data_dir, exist_ok=True)
        self.content_processor = content_processor
        self.file_processor = FileProcessor()
    
    def process_file(self, file_path: str, filename: str, subject: str) -> Dict[str, Any]:
        """è™•ç†æª”æ¡ˆçš„åŒæ­¥åŒ…è£æ–¹æ³•"""
        try:
            # ä½¿ç”¨æª”æ¡ˆè™•ç†å™¨è®€å–æª”æ¡ˆå…§å®¹
            content, file_type = self.file_processor.process_input(file_path)
            
            # å‘¼å«éåŒæ­¥è™•ç†æ–¹æ³•
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(
                    self.process_learning_material(content, subject, filename)
                )
                return result
            finally:
                loop.close()
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'questions': []
            }
    
    async def process_learning_material(self, raw_text: str, subject: str, source: str) -> Dict[str, Any]:
        """è™•ç†å­¸ç¿’è³‡æ–™çš„å®Œæ•´æµç¨‹"""
        try:
            # æ¸…ç†æ–‡å­—
            cleaned_text = ContentValidator.clean_text(raw_text)
            
            # 1. Summarizer - æ‘˜è¦å…¨æ–‡
            print("æ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
            summary_data = await self.gemini.generate_summary(cleaned_text)
            
            # 2. Tagger - ç”Ÿæˆæ¨™ç±¤
            print("æ­£åœ¨ç”Ÿæˆæ¨™ç±¤...")
            tags = await self.gemini.generate_tags(cleaned_text, subject)
            
            # 3. QAGenerator - ç”Ÿæˆæ¨¡æ“¬é¡Œ
            print("æ­£åœ¨ç”Ÿæˆæ¨¡æ“¬é¡Œ...")
            questions = await self.gemini.generate_questions(
                summary_data.get('bullets', [])
            )
            
            # ğŸ†• 4. æ–°å¢ï¼šç”Ÿæˆé‡é»æ‘˜è¦èˆ‡å¿«é€Ÿæ¸¬é©—é¸æ“‡é¡Œ
            print("æ­£åœ¨ç”Ÿæˆé‡é»æ‘˜è¦èˆ‡å¿«é€Ÿæ¸¬é©—...")
            key_points_summary = await self.gemini.generate_key_points_summary(cleaned_text)
            quick_quiz = await self.gemini.generate_quick_quiz(cleaned_text, subject)
            
            # 5. StorageAgent - å„²å­˜è³‡æ–™
            print("æ­£åœ¨å„²å­˜è³‡æ–™...")
            result = await self._store_info_data(
                raw_text=raw_text,
                cleaned_text=cleaned_text,
                summary_data=summary_data,
                subject=subject,
                tags=tags,
                questions=questions,
                source=source,
                key_points_summary=key_points_summary,  # ğŸ†• æ–°å¢åƒæ•¸
                quick_quiz=quick_quiz  # ğŸ†• æ–°å¢åƒæ•¸
            )
            
            # 6. Process content for knowledge points
            print("æ­£åœ¨è™•ç†å…§å®¹ä»¥æå–çŸ¥è­˜é»...")
            doc_id = result['document_id']
            doc_title = os.path.basename(result['file_path'])
            
            # ç§»é™¤å° content_processor çš„èª¿ç”¨ï¼Œå› ç‚ºå­¸ç¿’è³‡æ–™ä¸æ‡‰è©²è¢«ç•¶ä½œè€ƒé¡Œä¾†è§£æ
            # processing_result = await self.content_processor.process_content(
            #     text=cleaned_text,
            #     subject=subject,
            #     doc_title=doc_title,
            #     doc_id=doc_id
            # )
            # if not processing_result.get('success'):
            #      print(f"è­¦å‘Šï¼šç„¡æ³•è™•ç†å…§å®¹ä»¥æå–çŸ¥è­˜é»ï¼š{processing_result.get('error')}")

            # ç›´æ¥ä½¿ç”¨ Tagger ç”Ÿæˆçš„æ¨™ç±¤ä½œç‚ºçŸ¥è­˜é»
            knowledge_points = tags

            return {
                'success': True,
                'type': 'info',
                'subject': subject,
                'document_id': result['document_id'],
                'question_ids': result['question_ids'],
                'file_path': result['file_path'],
                'data': {
                    'summary': summary_data.get('summary', ''),
                    'bullets': summary_data.get('bullets', []),
                    'tags': tags,
                    'questions': questions,
                    'knowledge_points': knowledge_points,
                    'key_points_summary': key_points_summary,
                    'quick_quiz': quick_quiz
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'type': 'info'
            }
    
    async def _store_info_data(self, raw_text: str, cleaned_text: str,
                              summary_data: Dict[str, Any], subject: str,
                              tags: List[str], questions: List[Dict[str, Any]], source: str,
                              key_points_summary: str = "", quick_quiz: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å„²å­˜å­¸ç¿’è³‡æ–™"""
        
        if quick_quiz is None:
            quick_quiz = []
        
        # ç”Ÿæˆå”¯ä¸€æª”æ¡ˆåç¨±
        content_hash = hashlib.md5(cleaned_text.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{content_hash}.md"
        
        # å»ºç«‹ç§‘ç›®ç›®éŒ„
        subject_dir = os.path.join(self.data_dir, subject)
        os.makedirs(subject_dir, exist_ok=True)
        
        file_path = os.path.join(subject_dir, filename)
        
        # ç”Ÿæˆ Markdown å…§å®¹
        markdown_content = self._generate_info_markdown(
            original_text=cleaned_text,
            summary=summary_data.get('summary', ''),
            bullets=summary_data.get('bullets', []),
            tags=tags,
            questions=questions,
            subject=subject,
            source=source,
            key_points_summary=key_points_summary,  # ğŸ†• æ–°å¢åƒæ•¸
            quick_quiz=quick_quiz  # ğŸ†• æ–°å¢åƒæ•¸
        )
        
        # å¯«å…¥æª”æ¡ˆ
        FileProcessor.save_markdown(markdown_content, file_path)
        
        # å„²å­˜åˆ°è³‡æ–™åº« - å¢åŠ è³‡æ–™å‹æ…‹æª¢æŸ¥èˆ‡è½‰æ›
        import json
        
        # ç¢ºä¿ key_points_summary æ˜¯å­—ä¸²æ ¼å¼ï¼Œé˜²æ­¢ 'dict' object has no attribute 'replace' éŒ¯èª¤
        if isinstance(key_points_summary, dict):
            print("è­¦å‘Šï¼škey_points_summary æ˜¯å­—å…¸æ ¼å¼ï¼Œæ­£åœ¨è½‰æ›ç‚ºæ–‡å­—...")
            key_points_summary = json.dumps(key_points_summary, ensure_ascii=False, indent=2)
        elif key_points_summary is None:
            key_points_summary = ""
        else:
            key_points_summary = str(key_points_summary)  # ç¢ºä¿æ˜¯å­—ä¸²
        
        # è™•ç† quick_quiz çš„ JSON åºåˆ—åŒ–
        if isinstance(quick_quiz, list) and quick_quiz:
            quick_quiz_json = json.dumps(quick_quiz, ensure_ascii=False)
        else:
            quick_quiz_json = None
        
        doc_id = self.db.add_document(
            title=filename,
            content=cleaned_text,
            subject=subject,
            tags=",".join(tags),
            file_path=file_path,
            source=source,
            key_points_summary=key_points_summary,
            quick_quiz=quick_quiz_json
        )
        
        question_ids = []
        for q in questions:
            q_id = self.db.add_question(
                document_id=doc_id,
                question_text=q['stem'],
                answer_text=q['answer'],
                subject=subject
            )
            question_ids.append(q_id)
            
        return {
            'document_id': doc_id,
            'question_ids': question_ids,
            'file_path': file_path
        }
        
    def _generate_info_markdown(self, original_text: str, summary: str, bullets: List[str],
                                tags: List[str], questions: List[Dict[str, Any]], subject: str, source: str,
                                key_points_summary: str = "", quick_quiz: List[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆå­¸ç¿’è³‡æ–™çš„ Markdown æ ¼å¼å…§å®¹"""
        
        if quick_quiz is None:
            quick_quiz = []
        
        md_content = f"# {subject} å­¸ç¿’ç­†è¨˜\n\n"
        md_content += f"**ä¾†æº:** {source}\n"
        md_content += f"**æ¨™ç±¤:** {', '.join(tags)}\n\n"
        
        # ğŸ†• æ–°å¢ï¼šåŸå§‹å…¨æ–‡å€åŸŸ
        md_content += "## ğŸ“„ åŸå§‹æ–‡æœ¬\n"
        md_content += f"{original_text}\n\n"
        md_content += "---\n\n"
        
        # ğŸ†• æ–°å¢ï¼šé‡é»æ‘˜è¦å€åŸŸ
        if key_points_summary:
            md_content += "## â­ é‡é»æ‘˜è¦\n"
            md_content += f"{key_points_summary}\n\n"
            md_content += "---\n\n"
        
        # ğŸ†• æ–°å¢ï¼šå¿«é€Ÿæ¸¬é©—å€åŸŸ
        if quick_quiz:
            md_content += "## ğŸ¯ å¿«é€Ÿæ¸¬é©—\n"
            md_content += "*å¿«é€Ÿæª¢é©—æ‚¨å°é‡é»çŸ¥è­˜çš„æŒæ¡ç¨‹åº¦*\n\n"
            for i, q in enumerate(quick_quiz, 1):
                md_content += f"**{i}. {q.get('question', '')}**\n\n"
                
                # è™•ç†é¸æ“‡é¡Œé¸é …
                if q.get('type') == 'multiple_choice' and q.get('options'):
                    for opt in q['options']:
                        md_content += f"   {opt}\n"
                    md_content += f"\n   **æ­£è§£ï¼š{q.get('correct_answer', '')}**\n"
                    if q.get('explanation'):
                        md_content += f"   **è§£æï¼š{q.get('explanation', '')}**\n"
                elif q.get('type') == 'true_false':
                    md_content += f"   **æ­£è§£ï¼š{'æ˜¯' if q.get('correct_answer') else 'å¦'}**\n"
                    if q.get('explanation'):
                        md_content += f"   **è§£æï¼š{q.get('explanation', '')}**\n"
                else:
                    md_content += f"   **ç­”æ¡ˆï¼š{q.get('correct_answer', '')}**\n"
                    if q.get('explanation'):
                        md_content += f"   **è§£æï¼š{q.get('explanation', '')}**\n"
                md_content += "\n"
            md_content += "---\n\n"
        
        # åŸæœ‰çš„æ‘˜è¦å€åŸŸï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
        md_content += "## ğŸ“‹ å­¸ç¿’æ‘˜è¦\n"
        md_content += f"{summary}\n\n"
        
        md_content += "## ğŸ“ é‡é»æ•´ç†\n"
        for bullet in bullets:
            md_content += f"- {bullet}\n"
        md_content += "\n"
        
        # åŸæœ‰çš„æ¨¡æ“¬è©¦é¡Œå€åŸŸï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
        md_content += "## ğŸ“š æ¨¡æ“¬è©¦é¡Œ\n"
        md_content += "*æ·±åº¦ç†è§£èˆ‡æ‡‰ç”¨ç·´ç¿’*\n\n"
        for i, q in enumerate(questions, 1):
            md_content += f"**é¡Œç›® {i}:** {q['stem']}\n\n"
            md_content += f"**ç­”æ¡ˆ:**\n{q['answer']}\n\n"
        
        return md_content
